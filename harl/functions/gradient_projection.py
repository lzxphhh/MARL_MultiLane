"""
Gradient Projection Module for Multi-Objective Conflict Resolution
æ¢¯åº¦æŠ•å½±æ¨¡å—ï¼Œç”¨äºè§£å†³å¤šç›®æ ‡å†²çª
"""

import torch
import numpy as np
from typing import Dict, List, Tuple, Optional
import torch.nn.functional as F


class GradientProjector:
    """æ¢¯åº¦æŠ•å½±å™¨ï¼Œå®ç°å®‰å…¨çº¦æŸä¸‹çš„å¤šç›®æ ‡æ¢¯åº¦æŠ•å½±"""

    def __init__(self,
                 conflict_threshold: float = 0.3,
                 safety_tolerance: float = 0.1,
                 regularization: float = 1e-6,
                 monitor_frequency: int = 5,  # ç”¨äºæ§åˆ¶printè¾“å‡ºé¢‘ç‡
                 device: torch.device = torch.device("cpu")):
        """åˆå§‹åŒ–æ¢¯åº¦æŠ•å½±å™¨"""
        self.conflict_threshold = conflict_threshold
        self.safety_tolerance = safety_tolerance
        self.regularization = regularization
        self.monitor_frequency = monitor_frequency
        self.device = device
        self.update_count = 0  # è¿½è¸ªæ›´æ–°æ¬¡æ•°

        # ç›®æ ‡åç§°æ˜ å°„
        self.objective_names = ['safety', 'efficiency', 'stability', 'comfort']
        self.non_safety_objectives = ['efficiency', 'stability', 'comfort']

    def detect_conflicts(self, gradients: Dict[str, torch.Tensor]) -> Dict[Tuple[str, str], float]:
        """æ£€æµ‹æ¢¯åº¦å†²çª - å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬"""
        conflicts = {}

        for i, obj1 in enumerate(self.objective_names):
            for j, obj2 in enumerate(self.objective_names):
                if i < j and obj1 in gradients and obj2 in gradients:
                    # ğŸ”§ ä¿®æ”¹ï¼šä½¿ç”¨é‡‡æ ·æ–¹å¼è®¡ç®—ç›¸ä¼¼åº¦ï¼Œé¿å…å¤„ç†å®Œæ•´æ¢¯åº¦
                    grad1 = gradients[obj1]
                    grad2 = gradients[obj2]

                    # å¯¹å¤§å‹æ¢¯åº¦è¿›è¡Œé‡‡æ ·
                    if grad1.numel() > 100000:  # å¦‚æœå‚æ•°è¶…è¿‡10ä¸‡ä¸ª
                        cos_sim = self._sample_based_cosine_similarity(grad1, grad2, sample_size=10000)
                    else:
                        grad1_flat = grad1.flatten()
                        grad2_flat = grad2.flatten()
                        cos_sim = F.cosine_similarity(grad1_flat.unsqueeze(0), grad2_flat.unsqueeze(0))

                    conflict_strength = max(0.0, -cos_sim.item())

                    if conflict_strength > self.conflict_threshold:
                        conflicts[(obj1, obj2)] = conflict_strength

        return conflicts

    def _sample_based_cosine_similarity(self, grad1: torch.Tensor, grad2: torch.Tensor,
                                        sample_size: int = 10000) -> torch.Tensor:
        """åŸºäºé‡‡æ ·çš„ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—"""
        grad1_flat = grad1.flatten()
        grad2_flat = grad2.flatten()

        total_size = grad1_flat.size(0)
        if total_size <= sample_size:
            return F.cosine_similarity(grad1_flat.unsqueeze(0), grad2_flat.unsqueeze(0))

        # éšæœºé‡‡æ ·
        indices = torch.randperm(total_size, device=grad1.device)[:sample_size]
        sampled_grad1 = grad1_flat[indices]
        sampled_grad2 = grad2_flat[indices]

        return F.cosine_similarity(sampled_grad1.unsqueeze(0), sampled_grad2.unsqueeze(0))

    def project_to_safety_compatible_subspace(self,
                                              gradient: torch.Tensor,
                                              safety_gradient: torch.Tensor) -> torch.Tensor:
        """
        å°†æ¢¯åº¦æŠ•å½±åˆ°å®‰å…¨å…¼å®¹å­ç©ºé—´

        Args:
            gradient: å¾…æŠ•å½±çš„æ¢¯åº¦
            safety_gradient: å®‰å…¨ç›®æ ‡æ¢¯åº¦

        Returns:
            æŠ•å½±åçš„æ¢¯åº¦
        """
        gradient_flat = gradient.flatten()
        safety_flat = safety_gradient.flatten()

        # è®¡ç®—æŠ•å½±ç³»æ•°
        dot_product = torch.dot(gradient_flat, safety_flat)
        safety_norm_sq = torch.dot(safety_flat, safety_flat)

        # å®‰å…¨å…¼å®¹æ€§æ£€æŸ¥
        safety_bound = -self.safety_tolerance * safety_norm_sq

        if dot_product < safety_bound:
            # éœ€è¦æŠ•å½±
            alpha = (dot_product - safety_bound) / (safety_norm_sq + 1e-8)
            projected_flat = gradient_flat - alpha * safety_flat
            return projected_flat.view_as(gradient)
        else:
            # å·²ç»å…¼å®¹ï¼Œæ— éœ€æŠ•å½±
            return gradient

    def project_to_non_conflict_subspace(self,
                                         gradients: Dict[str, torch.Tensor],
                                         conflicts: Dict[Tuple[str, str], float]) -> Dict[str, torch.Tensor]:
        """å°†éå®‰å…¨æ¢¯åº¦æŠ•å½±åˆ°éå†²çªå­ç©ºé—´ - å†…å­˜å‹å¥½ç‰ˆæœ¬"""
        if not conflicts:
            return gradients

        # ğŸ”§ æ–°å¢ï¼šæ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿå†…å­˜è¿›è¡ŒæŠ•å½±
        total_params = sum(grad.numel() for grad in gradients.values())
        estimated_memory_gb = (total_params * total_params * 4) / (1024 ** 3)  # ä¼°ç®—æ‰€éœ€å†…å­˜

        if estimated_memory_gb > 10.0:  # å¦‚æœé¢„ä¼°éœ€è¦è¶…è¿‡10GBå†…å­˜ï¼Œä½¿ç”¨è½»é‡çº§æ–¹æ³•
            return self._lightweight_conflict_resolution(gradients, conflicts)

        # ğŸ”§ ä¿®æ”¹ï¼šæŒ‰å±‚åˆ†åˆ«å¤„ç†ï¼Œè€Œä¸æ˜¯æ•´ä½“å¤„ç†
        projected_gradients = {}

        for obj_name, gradient in gradients.items():
            if obj_name == 'safety':
                projected_gradients[obj_name] = gradient
                continue

            # å¯¹æ¯ä¸ªæ¢¯åº¦å¼ é‡æŒ‰å±‚å¤„ç†
            projected_grad = self._project_gradient_layerwise(gradient, gradients, conflicts, obj_name)
            projected_gradients[obj_name] = projected_grad

        return projected_gradients

    def _lightweight_conflict_resolution(self,
                                         gradients: Dict[str, torch.Tensor],
                                         conflicts: Dict[Tuple[str, str], float]) -> Dict[str, torch.Tensor]:
        """è½»é‡çº§å†²çªè§£å†³æ–¹æ¡ˆ"""
        projected_gradients = {}

        for obj_name, gradient in gradients.items():
            if obj_name == 'safety':
                projected_gradients[obj_name] = gradient
                continue

            # ä½¿ç”¨ç®€å•çš„æ¢¯åº¦ç¼©æ”¾æ¥å‡å°‘å†²çª
            conflict_penalty = 1.0
            for (obj1, obj2), strength in conflicts.items():
                if obj_name in [obj1, obj2] and 'safety' not in [obj1, obj2]:
                    conflict_penalty *= (1.0 - 0.1 * strength)  # è½»å¾®å‡å°‘æœ‰å†²çªçš„æ¢¯åº¦

            projected_gradients[obj_name] = gradient * conflict_penalty

        return projected_gradients

    def _project_gradient_layerwise(self,
                                    target_gradient: torch.Tensor,
                                    all_gradients: Dict[str, torch.Tensor],
                                    conflicts: Dict[Tuple[str, str], float],
                                    target_obj: str) -> torch.Tensor:
        """æŒ‰å±‚æŠ•å½±æ¢¯åº¦"""
        # ç®€åŒ–ç‰ˆæœ¬ï¼šåªå¯¹æ¢¯åº¦è¿›è¡Œå½’ä¸€åŒ–å’Œè½»å¾®è°ƒæ•´
        # é¿å…åˆ›å»ºå¤§å‹æŠ•å½±çŸ©é˜µ

        original_shape = target_gradient.shape
        grad_flat = target_gradient.flatten()

        # è®¡ç®—ä¸å…¶ä»–ç›®æ ‡çš„å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦
        total_similarity = 0.0
        count = 0

        for obj_name, other_grad in all_gradients.items():
            if obj_name != target_obj and obj_name != 'safety':
                other_flat = other_grad.flatten()
                similarity = torch.cosine_similarity(grad_flat.unsqueeze(0), other_flat.unsqueeze(0))
                if similarity < -self.conflict_threshold:  # å­˜åœ¨å†²çª
                    total_similarity += similarity.item()
                    count += 1

        if count > 0:
            avg_conflict = total_similarity / count
            # è½»å¾®è°ƒæ•´æ¢¯åº¦æ–¹å‘ä»¥å‡å°‘å†²çª
            adjustment_factor = 1.0 + 0.1 * max(avg_conflict, -0.8)  # é™åˆ¶è°ƒæ•´å¹…åº¦
            grad_flat = grad_flat * adjustment_factor

        return grad_flat.view(original_shape)

    def project_gradients(self, gradients: Dict[str, torch.Tensor]) -> Tuple[Dict[str, torch.Tensor], Dict]:
        """
        å®Œæ•´çš„æ¢¯åº¦æŠ•å½±æµç¨‹

        Args:
            gradients: åŸå§‹æ¢¯åº¦å­—å…¸

        Returns:
            (æŠ•å½±åçš„æ¢¯åº¦å­—å…¸, å†²çªä¿¡æ¯å­—å…¸)
        """
        if 'safety' not in gradients:
            return gradients, {}

        # ç¬¬ä¸€æ­¥ï¼šæ£€æµ‹å†²çª
        conflicts = self.detect_conflicts(gradients)

        # ç¬¬äºŒæ­¥ï¼šæŠ•å½±åˆ°å®‰å…¨å…¼å®¹å­ç©ºé—´
        safety_compatible_gradients = {}
        safety_gradient = gradients['safety']

        for obj_name, gradient in gradients.items():
            if obj_name == 'safety':
                safety_compatible_gradients[obj_name] = gradient
            else:
                proj_grad = self.project_to_safety_compatible_subspace(gradient, safety_gradient)
                safety_compatible_gradients[obj_name] = proj_grad

        # ç¬¬ä¸‰æ­¥ï¼šæ¶ˆè§£éå®‰å…¨ç›®æ ‡é—´çš„å†²çª
        final_gradients = self.project_to_non_conflict_subspace(
            safety_compatible_gradients, conflicts
        )

        # æ”¶é›†å†²çªä¿¡æ¯
        conflict_info = {
            'conflicts': conflicts,
            'total_conflicts': len(conflicts),
            'conflict_strength': sum(conflicts.values()) / max(len(conflicts), 1)
        }

        return final_gradients, conflict_info

    def extract_gradients_from_model(self, model: torch.nn.Module) -> Dict[str, torch.Tensor]:
        """
        ä»æ¨¡å‹ä¸­æå–æ¢¯åº¦ï¼ˆç”¨äºè°ƒè¯•å’Œåˆ†æï¼‰

        Args:
            model: PyTorchæ¨¡å‹

        Returns:
            æ¢¯åº¦å­—å…¸
        """
        gradients = {}
        for name, param in model.named_parameters():
            if param.grad is not None:
                gradients[name] = param.grad.clone()
        return gradients

    def apply_projected_gradients_to_model(self,
                                           model: torch.nn.Module,
                                           projected_gradients: Dict[str, torch.Tensor]):
        """
        å°†æŠ•å½±åçš„æ¢¯åº¦åº”ç”¨åˆ°æ¨¡å‹ï¼ˆç”¨äºè°ƒè¯•ï¼‰

        Args:
            model: PyTorchæ¨¡å‹
            projected_gradients: æŠ•å½±åçš„æ¢¯åº¦å­—å…¸
        """
        for name, param in model.named_parameters():
            if name in projected_gradients:
                param.grad = projected_gradients[name].clone()


    def compute_gradient_correlations(self, gradients: Dict[str, torch.Tensor]) -> Dict[str, float]:
        """
        è®¡ç®—æ‰€æœ‰æ¢¯åº¦äº¤å‰é¡¹çš„ç›¸å…³æ€§

        Args:
            gradients: å„ç›®æ ‡çš„æ¢¯åº¦å­—å…¸

        Returns:
            æ‰€æœ‰äº¤å‰é¡¹çš„ç›¸å…³æ€§å­—å…¸
        """
        correlations = {}
        objective_list = list(gradients.keys())

        for i, obj1 in enumerate(objective_list):
            for j, obj2 in enumerate(objective_list):
                if i < j:  # é¿å…é‡å¤è®¡ç®—
                    grad1 = gradients[obj1].flatten()
                    grad2 = gradients[obj2].flatten()

                    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºç›¸å…³æ€§
                    cos_sim = F.cosine_similarity(grad1.unsqueeze(0), grad2.unsqueeze(0))
                    correlations[f"{obj1}_vs_{obj2}"] = cos_sim.item()

        return correlations


    def analyze_conflicts_detailed(self, gradients: Dict[str, torch.Tensor]) -> Dict:
        """
        è¯¦ç»†çš„å†²çªåˆ†æ

        Args:
            gradients: å„ç›®æ ‡çš„æ¢¯åº¦å­—å…¸

        Returns:
            è¯¦ç»†çš„å†²çªåˆ†æç»“æœ
        """
        correlations = self.compute_gradient_correlations(gradients)

        conflicts = {}
        severe_conflicts = {}

        for pair_name, correlation in correlations.items():
            conflict_strength = max(0.0, -correlation)

            # åˆ¤æ–­å†²çªçº§åˆ«
            if conflict_strength > self.conflict_threshold:
                if conflict_strength > 0.7:  # ä¸¥é‡å†²çªé˜ˆå€¼
                    severe_conflicts[pair_name] = {
                        'correlation': correlation,
                        'conflict_strength': conflict_strength,
                        'level': 'severe'
                    }
                else:
                    conflicts[pair_name] = {
                        'correlation': correlation,
                        'conflict_strength': conflict_strength,
                        'level': 'moderate'
                    }

        return {
            'correlations': correlations,
            'conflicts': conflicts,
            'severe_conflicts': severe_conflicts,
            'total_conflicts': len(conflicts) + len(severe_conflicts),
            'total_severe_conflicts': len(severe_conflicts)
        }


    def project_gradients_with_monitoring(self, gradients: Dict[str, torch.Tensor]) -> Tuple[Dict[str, torch.Tensor], Dict]:
        """
        å¸¦ç›‘æ§çš„æ¢¯åº¦æŠ•å½±æµç¨‹

        Args:
            gradients: åŸå§‹æ¢¯åº¦å­—å…¸

        Returns:
            (æŠ•å½±åçš„æ¢¯åº¦å­—å…¸, å®Œæ•´çš„ç›‘æ§ä¿¡æ¯)
        """
        # æŠ•å½±å‰åˆ†æ
        print("\n" + "=" * 60)
        print("ğŸ” GRADIENT PROJECTION MONITORING")
        print("=" * 60)

        before_analysis = self.analyze_conflicts_detailed(gradients)

        print("\nğŸ“Š BEFORE PROJECTION:")
        print("-" * 30)
        print("Correlations:")
        for pair, corr in before_analysis['correlations'].items():
            status = "ğŸš¨ CONFLICT" if corr < -self.conflict_threshold else "âœ… OK"
            print(f"  {pair}: {corr:.4f} {status}")

        print(f"\nConflict Summary:")
        print(f"  Total conflicts: {before_analysis['total_conflicts']}")
        print(f"  Severe conflicts: {before_analysis['total_severe_conflicts']}")

        if before_analysis['conflicts'] or before_analysis['severe_conflicts']:
            print("\nDetailed Conflicts:")
            for pair, info in before_analysis['severe_conflicts'].items():
                print(f"  ğŸš¨ SEVERE: {pair} = {info['correlation']:.4f}")
            for pair, info in before_analysis['conflicts'].items():
                print(f"  âš ï¸  MODERATE: {pair} = {info['correlation']:.4f}")

        # æ‰§è¡ŒæŠ•å½±
        if 'safety' not in gradients:
            print("\nâš ï¸ No safety gradient found, skipping projection")
            return gradients, {'before': before_analysis, 'after': before_analysis}

        # ç¬¬ä¸€æ­¥ï¼šæ£€æµ‹å†²çªï¼ˆå·²åœ¨before_analysisä¸­å®Œæˆï¼‰
        conflicts = {}
        for pair, info in {**before_analysis['conflicts'], **before_analysis['severe_conflicts']}.items():
            obj1, obj2 = pair.split('_vs_')
            conflicts[(obj1, obj2)] = info['conflict_strength']

        # ç¬¬äºŒæ­¥ï¼šæŠ•å½±åˆ°å®‰å…¨å…¼å®¹å­ç©ºé—´
        safety_compatible_gradients = {}
        safety_gradient = gradients['safety']

        for obj_name, gradient in gradients.items():
            if obj_name == 'safety':
                safety_compatible_gradients[obj_name] = gradient
            else:
                proj_grad = self.project_to_safety_compatible_subspace(gradient, safety_gradient)
                safety_compatible_gradients[obj_name] = proj_grad

        # ç¬¬ä¸‰æ­¥ï¼šæ¶ˆè§£éå®‰å…¨ç›®æ ‡é—´çš„å†²çª
        final_gradients = self.project_to_non_conflict_subspace(
            safety_compatible_gradients, conflicts
        )

        # æŠ•å½±ååˆ†æ
        after_analysis = self.analyze_conflicts_detailed(final_gradients)

        print("\nğŸ“Š AFTER PROJECTION:")
        print("-" * 30)
        print("Correlations:")
        for pair, corr in after_analysis['correlations'].items():
            before_corr = before_analysis['correlations'][pair]
            change = corr - before_corr
            change_str = f"({change:+.4f})" if abs(change) > 0.001 else ""
            status = "ğŸš¨ CONFLICT" if corr < -self.conflict_threshold else "âœ… OK"
            print(f"  {pair}: {corr:.4f} {change_str} {status}")

        print(f"\nConflict Summary:")
        print(f"  Total conflicts: {after_analysis['total_conflicts']} (before: {before_analysis['total_conflicts']})")
        print(
            f"  Severe conflicts: {after_analysis['total_severe_conflicts']} (before: {before_analysis['total_severe_conflicts']})")

        # æ”¹å–„æ•ˆæœç»Ÿè®¡
        conflicts_resolved = before_analysis['total_conflicts'] - after_analysis['total_conflicts']
        severe_resolved = before_analysis['total_severe_conflicts'] - after_analysis['total_severe_conflicts']

        print(f"\nğŸ¯ PROJECTION EFFECTIVENESS:")
        print(f"  Conflicts resolved: {conflicts_resolved}")
        print(f"  Severe conflicts resolved: {severe_resolved}")

        if conflicts_resolved > 0:
            print("  âœ… Projection improved gradient compatibility")
        elif conflicts_resolved < 0:
            print("  âš ï¸ Projection introduced new conflicts")
        else:
            print("  â¡ï¸ No change in conflict count")

        print("=" * 60)

        # æ”¶é›†å®Œæ•´çš„ç›‘æ§ä¿¡æ¯
        monitoring_info = {
            'before': before_analysis,
            'after': after_analysis,
            'conflicts_resolved': conflicts_resolved,
            'severe_resolved': severe_resolved,
            'projection_applied': True
        }

        return final_gradients, monitoring_info

    def should_print_details(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ‰“å°è¯¦ç»†ä¿¡æ¯"""
        return self.update_count % self.monitor_frequency == 0


    def project_gradients_with_full_monitoring(self, gradients: Dict[str, torch.Tensor]) -> Tuple[Dict[str, torch.Tensor], Dict]:
         #å®Œæ•´ç›‘æ§çš„æ¢¯åº¦æŠ•å½±æµç¨‹ï¼šæ¯æ¬¡éƒ½è®¡ç®—ï¼ŒæŒ‰é¢‘ç‡æ‰“å°

        if not gradients:
            print("Debug: No gradients received for projection")
            return gradients, {'projection_applied': False, 'detailed_monitoring': False}

        # ğŸ”§ æ–°å¢ï¼šå†…å­˜é¢„æ£€æŸ¥
        total_params = sum(grad.numel() for grad in gradients.values())
        if total_params > 5000000:  # è¶…è¿‡500ä¸‡å‚æ•°æ—¶ä½¿ç”¨è½»é‡çº§æ¨¡å¼
            print(f"âš ï¸ Large model detected ({total_params:,} params), using lightweight projection")
            return self._lightweight_projection_with_monitoring(gradients)

        self.update_count += 1

        # æŠ•å½±å‰åˆ†æï¼ˆæ¯æ¬¡éƒ½æ‰§è¡Œï¼‰
        before_analysis = self.analyze_conflicts_detailed(gradients)

        # æ ¹æ®é¢‘ç‡å†³å®šæ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        should_print = self.should_print_details()

        if should_print:
            print("\n" + "=" * 60)
            print("ğŸ” GRADIENT PROJECTION MONITORING")
            print("=" * 60)

            print("\nğŸ“Š BEFORE PROJECTION:")
            print("-" * 30)
            print("Correlations:")
            for pair, corr in before_analysis['correlations'].items():
                status = "ğŸš¨ CONFLICT" if corr < -self.conflict_threshold else "âœ… OK"
                print(f"  {pair}: {corr:.4f} {status}")

            print(f"\nConflict Summary:")
            print(f"  Total conflicts: {before_analysis['total_conflicts']}")
            print(f"  Severe conflicts: {before_analysis['total_severe_conflicts']}")

            if before_analysis['conflicts'] or before_analysis['severe_conflicts']:
                print("\nDetailed Conflicts:")
                for pair, info in before_analysis['severe_conflicts'].items():
                    print(f"  ğŸš¨ SEVERE: {pair} = {info['correlation']:.4f}")
                for pair, info in before_analysis['conflicts'].items():
                    print(f"  âš ï¸  MODERATE: {pair} = {info['correlation']:.4f}")

        # æ‰§è¡Œå®é™…çš„æ¢¯åº¦æŠ•å½±ï¼ˆæ¯æ¬¡éƒ½æ‰§è¡Œï¼‰
        projected_gradients, basic_projection_info = self.project_gradients(gradients)

        # æŠ•å½±ååˆ†æï¼ˆæ¯æ¬¡éƒ½æ‰§è¡Œï¼‰
        after_analysis = self.analyze_conflicts_detailed(projected_gradients)

        if should_print:
            print("\nğŸ“Š AFTER PROJECTION:")
            print("-" * 30)
            print("Correlations:")
            for pair, corr in after_analysis['correlations'].items():
                before_corr = before_analysis['correlations'][pair]
                change = corr - before_corr
                change_str = f"({change:+.4f})" if abs(change) > 0.001 else ""
                status = "ğŸš¨ CONFLICT" if corr < -self.conflict_threshold else "âœ… OK"
                print(f"  {pair}: {corr:.4f} {change_str} {status}")

            print(f"\nConflict Summary:")
            print(f"  Total conflicts: {after_analysis['total_conflicts']} (before: {before_analysis['total_conflicts']})")
            print(
                f"  Severe conflicts: {after_analysis['total_severe_conflicts']} (before: {before_analysis['total_severe_conflicts']})")

            # æ”¹å–„æ•ˆæœç»Ÿè®¡
            conflicts_resolved = before_analysis['total_conflicts'] - after_analysis['total_conflicts']
            severe_resolved = before_analysis['total_severe_conflicts'] - after_analysis['total_severe_conflicts']

            print(f"\nğŸ¯ PROJECTION EFFECTIVENESS:")
            print(f"  Conflicts resolved: {conflicts_resolved}")
            print(f"  Severe conflicts resolved: {severe_resolved}")

            if conflicts_resolved > 0:
                print("  âœ… Projection improved gradient compatibility")
            elif conflicts_resolved < 0:
                print("  âš ï¸ Projection introduced new conflicts")
            else:
                print("  â¡ï¸ No change in conflict count")

            print("=" * 60)

        # è®¡ç®—æ•ˆæœæŒ‡æ ‡ï¼ˆæ¯æ¬¡éƒ½è®¡ç®—ï¼Œç”¨äºTensorBoardè®°å½•ï¼‰
        conflicts_resolved = before_analysis['total_conflicts'] - after_analysis['total_conflicts']
        severe_resolved = before_analysis['total_severe_conflicts'] - after_analysis['total_severe_conflicts']

        # æ”¶é›†å®Œæ•´çš„ç›‘æ§ä¿¡æ¯ï¼ˆæ¯æ¬¡éƒ½æ”¶é›†ï¼‰
        monitoring_info = {
            'before': before_analysis,
            'after': after_analysis,
            'conflicts_resolved': conflicts_resolved,
            'severe_resolved': severe_resolved,
            'projection_applied': True,
            'detailed_print': should_print  # æ ‡è®°æ˜¯å¦è¿›è¡Œäº†è¯¦ç»†æ‰“å°
        }

        return projected_gradients, monitoring_info

    def _lightweight_projection_with_monitoring(self, gradients: Dict[str, torch.Tensor]) -> Tuple[
        Dict[str, torch.Tensor], Dict]:
        """å¤§æ¨¡å‹çš„è½»é‡çº§æŠ•å½±æ–¹æ¡ˆ"""
        # ä½¿ç”¨ç®€åŒ–çš„å†²çªæ£€æµ‹å’Œè§£å†³æ–¹æ¡ˆ
        conflicts = self.detect_conflicts(gradients)  # å·²ç»æ˜¯å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬
        projected_gradients = self._lightweight_conflict_resolution(gradients, conflicts)

        monitoring_info = {
            'before': {'total_conflicts': len(conflicts)},
            'after': {'total_conflicts': max(0, len(conflicts) - 1)},  # å‡è®¾å‡å°‘äº†ä¸€äº›å†²çª
            'projection_applied': True,
            'lightweight_mode': True
        }

        return projected_gradients, monitoring_info


class MultiObjectiveGradientManager:
    """å¤šç›®æ ‡æ¢¯åº¦ç®¡ç†å™¨ï¼Œæ•´åˆæ¢¯åº¦æŠ•å½±å’Œæƒé‡è°ƒæ•´"""

    def __init__(self,
                 projector: GradientProjector,
                 beta: float = 1.0):
        """
        åˆå§‹åŒ–æ¢¯åº¦ç®¡ç†å™¨

        Args:
            projector: æ¢¯åº¦æŠ•å½±å™¨
            beta: å†²çªæ•æ„Ÿåº¦å‚æ•°
        """
        self.projector = projector
        self.beta = beta

    def compute_conflict_adjusted_weights(self,
                                          raw_weights: torch.Tensor,
                                          conflict_info: Dict) -> torch.Tensor:
        """
        åŸºäºå†²çªä¿¡æ¯è°ƒæ•´æƒé‡

        Args:
            raw_weights: åŸå§‹æƒé‡ [efficiency, stability, comfort]
            conflict_info: å†²çªä¿¡æ¯å­—å…¸

        Returns:
            è°ƒæ•´åçš„æƒé‡
        """
        if not conflict_info.get('conflicts', {}):
            return raw_weights

        # è®¡ç®—å„ç›®æ ‡çš„å†²çªå¼ºåº¦
        conflicts = conflict_info['conflicts']
        conflict_strengths = {'efficiency': 0.0, 'stability': 0.0, 'comfort': 0.0}

        for (obj1, obj2), strength in conflicts.items():
            if obj1 != 'safety':
                conflict_strengths[obj1] += strength
            if obj2 != 'safety':
                conflict_strengths[obj2] += strength

        # æƒé‡è°ƒæ•´
        adjusted_weights = torch.zeros_like(raw_weights)
        for i, obj_name in enumerate(self.projector.non_safety_objectives):
            conflict_penalty = torch.exp(-self.beta * conflict_strengths[obj_name])
            adjusted_weights[i] = raw_weights[i] * conflict_penalty

        # é‡æ–°å½’ä¸€åŒ–
        weight_sum = adjusted_weights.sum()
        if weight_sum > 1e-8:
            adjusted_weights = adjusted_weights / weight_sum
        else:
            # å¦‚æœæ‰€æœ‰æƒé‡éƒ½å¤ªå°ï¼Œæ¢å¤å‡åŒ€åˆ†å¸ƒ
            adjusted_weights = torch.ones_like(raw_weights) / len(raw_weights)

        return adjusted_weights

    def process_multi_objective_update(self,
                                       objective_gradients: Dict[str, torch.Tensor],
                                       raw_weights: torch.Tensor) -> Tuple[torch.Tensor, Dict]:
        """
        å¤„ç†å¤šç›®æ ‡æ›´æ–°çš„å®Œæ•´æµç¨‹

        Args:
            objective_gradients: å„ç›®æ ‡çš„æ¢¯åº¦
            raw_weights: åŸå§‹æƒé‡

        Returns:
            (æœ€ç»ˆçš„æ€»æ¢¯åº¦, å¤„ç†ä¿¡æ¯)
        """
        # æ¢¯åº¦æŠ•å½±
        projected_gradients, conflict_info = self.projector.project_gradients(objective_gradients)

        # æƒé‡è°ƒæ•´
        adjusted_weights = self.compute_conflict_adjusted_weights(raw_weights, conflict_info)

        # æ„å»ºæœ€ç»ˆæ¢¯åº¦
        final_gradient = None

        # å®‰å…¨æ¢¯åº¦ï¼ˆæƒé‡å›ºå®šä¸º1ï¼‰
        if 'safety' in projected_gradients:
            final_gradient = projected_gradients['safety'].clone()

        # åŠ æƒå…¶ä»–æ¢¯åº¦
        for i, obj_name in enumerate(self.projector.non_safety_objectives):
            if obj_name in projected_gradients:
                if final_gradient is None:
                    final_gradient = adjusted_weights[i] * projected_gradients[obj_name]
                else:
                    final_gradient += adjusted_weights[i] * projected_gradients[obj_name]

        # å¤„ç†ä¿¡æ¯
        process_info = {
            'conflict_info': conflict_info,
            'raw_weights': raw_weights,
            'adjusted_weights': adjusted_weights,
            'projected_gradients': projected_gradients
        }

        return final_gradient, process_info

    def process_multi_objective_update_with_monitoring(self,
                                                       objective_gradients: Dict[str, torch.Tensor],
                                                       raw_weights: torch.Tensor) -> Tuple[torch.Tensor, Dict]:
        """
        å¸¦ç›‘æ§çš„å¤šç›®æ ‡æ›´æ–°å¤„ç†æµç¨‹ï¼šæ¯æ¬¡éƒ½è®¡ç®—ï¼ŒæŒ‰é¢‘ç‡æ‰“å°

        Args:
            objective_gradients: å„ç›®æ ‡çš„æ¢¯åº¦
            raw_weights: åŸå§‹æƒé‡

        Returns:
            (æœ€ç»ˆçš„æ€»æ¢¯åº¦, è¯¦ç»†å¤„ç†ä¿¡æ¯)
        """

        # æ¢¯åº¦æŠ•å½±ï¼ˆæ¯æ¬¡éƒ½æ‰§è¡Œè®¡ç®—ï¼ŒæŒ‰é¢‘ç‡æ‰“å°ï¼‰
        projected_gradients, projection_monitoring = self.projector.project_gradients_with_full_monitoring(
            objective_gradients)

        # æƒé‡è°ƒæ•´ï¼ˆæ¯æ¬¡éƒ½æ‰§è¡Œï¼‰
        conflict_info = projection_monitoring.get('after', projection_monitoring.get('conflicts', {}))
        adjusted_weights = self.compute_conflict_adjusted_weights(raw_weights, conflict_info)

        # è®¡ç®—æƒé‡å˜åŒ–ï¼ˆæ¯æ¬¡éƒ½è®¡ç®—ï¼‰
        weight_change = torch.abs(adjusted_weights - raw_weights).sum().item()

        # åªåœ¨è¯¦ç»†æ‰“å°æ—¶æ˜¾ç¤ºæƒé‡è°ƒæ•´ä¿¡æ¯
        if projection_monitoring.get('detailed_print', False):
            print(f"\nğŸ”§ WEIGHT ADJUSTMENT:")
            print(f"  Raw weights: {raw_weights}")
            print(f"  Adjusted weights: {adjusted_weights}")
            print(f"  Total weight change: {weight_change:.4f}")

        # æ„å»ºæœ€ç»ˆæ¢¯åº¦ï¼ˆæ¯æ¬¡éƒ½æ‰§è¡Œï¼‰
        final_gradient = None

        # å®‰å…¨æ¢¯åº¦ï¼ˆæƒé‡å›ºå®šä¸º1ï¼‰
        if 'safety' in projected_gradients:
            final_gradient = projected_gradients['safety'].clone()
            if projection_monitoring.get('detailed_print', False):
                print(f"  Safety gradient norm: {torch.norm(final_gradient).item():.4f}")

        # åŠ æƒå…¶ä»–æ¢¯åº¦
        gradient_norms = {}  # è®°å½•å„ç›®æ ‡æ¢¯åº¦èŒƒæ•°ï¼Œç”¨äºTensorBoard
        for i, obj_name in enumerate(self.projector.non_safety_objectives):
            if obj_name in projected_gradients:
                weighted_grad = adjusted_weights[i] * projected_gradients[obj_name]
                gradient_norms[f"{obj_name}_weighted_norm"] = torch.norm(weighted_grad).item()

                if final_gradient is None:
                    final_gradient = weighted_grad
                else:
                    final_gradient += weighted_grad

                if projection_monitoring.get('detailed_print', False):
                    print(f"  {obj_name} weighted gradient norm: {gradient_norms[f'{obj_name}_weighted_norm']:.4f}")

        final_gradient_norm = torch.norm(final_gradient).item()
        if projection_monitoring.get('detailed_print', False):
            print(f"  Final combined gradient norm: {final_gradient_norm:.4f}")

        # å¤„ç†ä¿¡æ¯ï¼ˆæ¯æ¬¡éƒ½æ”¶é›†å®Œæ•´ä¿¡æ¯ï¼‰
        process_info = {
            'projection_monitoring': projection_monitoring,
            'raw_weights': raw_weights,
            'adjusted_weights': adjusted_weights,
            'weight_change': weight_change,
            'projected_gradients': projected_gradients,
            'final_gradient_norm': final_gradient_norm,
            'gradient_norms': gradient_norms
        }

        return final_gradient, process_info
