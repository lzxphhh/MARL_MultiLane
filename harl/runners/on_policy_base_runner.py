"""Base runner for on-policy algorithms."""

import time
import os
import numpy as np
import torch
import setproctitle
from harl.common.valuenorm import ValueNorm
from harl.common.buffers.on_policy_actor_buffer import OnPolicyActorBuffer
from harl.common.buffers.on_policy_critic_buffer_ep import OnPolicyCriticBufferEP
from harl.common.buffers.on_policy_critic_buffer_fp import OnPolicyCriticBufferFP
from harl.algorithms.actors import ALGO_REGISTRY
from harl.algorithms.critics.v_critic import VCritic
from harl.utils.trans_tools import _t2n
from harl.utils.envs_tools import (
    make_eval_env,
    make_train_env,
    make_render_env,
    set_seed,
    get_num_agents,
)
from harl.utils.models_tools import init_device
from harl.utils.configs_tools import init_dir, save_config
from harl.utils.gradient_monitor import (
    GradientMonitor,
    NetworkAnalyzer,
)
from harl.envs import LOGGER_REGISTRY
import time

class OnPolicyBaseRunner:
    """Base runner for on-policy algorithms."""

    def __init__(self, args, algo_args, env_args):
        """Initialize the OnPolicyBaseRunner class.
        Args:
            args: command-line arguments parsed by argparse. Three keys: algo, env, exp_name.
            algo_args: arguments related to algo, loaded from config file and updated with unparsed command-line arguments.
            env_args: arguments related to env, loaded from config file and updated with unparsed command-line arguments.
        """
        self.args = args
        self.algo_args = algo_args
        self.env_args = env_args

        # è¯»å–ç®—æ³•ç›¸å…³config
        self.hidden_sizes = algo_args["model"]["hidden_sizes"]  # MLPéšè—å±‚ç¥žç»å…ƒæ•°é‡
        self.rnn_hidden_size = self.hidden_sizes[-1]  # RNNéšè—å±‚ç¥žç»å…ƒæ•°é‡
        self.recurrent_n = algo_args["model"]["recurrent_n"]  # RNNçš„å±‚æ•°
        self.action_aggregation = algo_args["algo"]["action_aggregation"]  # å¤šç»´åŠ¨ä½œç©ºé—´çš„èšåˆæ–¹å¼ï¼Œå¦‚mean/prod
        self.share_param = algo_args["algo"]["share_param"]  # actoræ˜¯å¦å…±äº«å‚æ•°
        self.fixed_order = algo_args["algo"]["fixed_order"]  # æ˜¯å¦å›ºå®šagentçš„ç­–ç•¥æ›´æ–°é¡ºåº
        set_seed(algo_args["seed"])  # è®¾ç½®éšæœºç§å­
        self.device = init_device(algo_args["device"])  # è®¾ç½®è®¾å¤‡

        # train, not render è¯´æ˜Žåœ¨è®­ç»ƒï¼Œä¸åœ¨eval
        if not self.algo_args["render"]["use_render"]:
            # åˆå§‹åŒ–è¿è¡Œè·¯å¾„ï¼Œæ—¥å¿—è·¯å¾„ï¼Œä¿å­˜è·¯å¾„ï¼Œtensorboardè·¯å¾„
            self.run_dir, self.log_dir, self.save_dir, self.writter = init_dir(
                args["env"],
                env_args,
                args["algo"],
                args["exp_name"],
                args["test_desc"],
                algo_args["seed"]["seed"],
                logger_path=algo_args["logger"]["log_dir"],
            )
            # ä¿å­˜algoï¼Œenv argsï¼Œalgo argsæ‰€æœ‰config
            save_config(args, algo_args, env_args, self.run_dir)

        # set the title of the process
        # è®¾ç½®è¿›ç¨‹çš„æ ‡é¢˜
        setproctitle.setproctitle(
            str(args["algo"]) + "-" + str(args["env"]) + "-" + str(args["exp_name"])
        )

        # ä½¿ç”¨env toolsä¸­çš„å‡½æ•°åˆ›å»ºè®­ç»ƒ/æµ‹è¯•/renderçŽ¯å¢ƒ ï¼ˆè°ƒå–çŽ¯å¢ƒ+æ’å…¥env configï¼‰
        if self.algo_args["render"]["use_render"]:
            # åˆ›å»ºå•çº¿ç¨‹renderçŽ¯å¢ƒ
            (
                self.envs,
                self.manual_render,
                self.manual_expand_dims,
                self.manual_delay,
                self.env_num,
            ) = make_render_env(args["env"], algo_args["seed"]["seed"], env_args)
        else:
            # åˆ›å»ºå¤šçº¿ç¨‹è®­ç»ƒçŽ¯å¢ƒ
            self.envs = make_train_env(
                args["env"],
                algo_args["seed"]["seed"],
                algo_args["train"]["n_rollout_threads"],
                env_args,
            )
            # åˆ›å»ºå¤šçº¿ç¨‹æµ‹è¯•çŽ¯å¢ƒ
            self.eval_envs = (
                make_eval_env(
                    args["env"],
                    algo_args["seed"]["seed"],
                    algo_args["eval"]["n_eval_rollout_threads"],
                    env_args,
                )
                if algo_args["eval"]["use_eval"]
                else None
            )
        # é»˜è®¤ä½¿ç”¨EPä½œä¸ºstate_type
        # EPï¼šEnvironmentProvided global state (EP)ï¼šçŽ¯å¢ƒæä¾›çš„å…¨å±€çŠ¶æ€
        # FPï¼šFeatured-Pruned Agent-Specific Global State (FP)ï¼š ç‰¹å¾è£å‰ªçš„ç‰¹å®šæ™ºèƒ½ä½“å…¨å±€çŠ¶æ€(ä¸åŒagentçš„å…¨å±€çŠ¶æ€ä¸åŒ, éœ€è¦agent number)
        self.state_type = env_args.get("state_type", "EP")
        # TODOï¼š EP or FP need to be added to customized env

        # æ™ºèƒ½ä½“æ•°é‡
        self.num_agents = get_num_agents(args["env"], env_args, self.envs)

        print("share_observation_space: ", self.envs.share_observation_space)
        print("observation_space: ", self.envs.observation_space)
        print("action_space: ", self.envs.action_space)
        print("num_agents: ", self.num_agents)
        # å¥–åŠ±ç»´åº¦
        self.num_reward_head = env_args["num_reward_head"]

        # actorç›¸å…³
        # actorå…±äº«å‚æ•°
        if self.share_param:
            self.actor = []
            # åˆå§‹åŒ–actorç½‘ç»œï¼Œè¿›å…¥mappo.py
            agent = ALGO_REGISTRY[args["algo"]](
                {**algo_args["model"], **algo_args["algo"], **algo_args["train"], **env_args},  # yamlé‡Œmodelå’Œalgoçš„configæ‰“åŒ…ä½œä¸ºargsè¿›å…¥OnPolicyBase
                self.envs.observation_space[0], # å•ä¸ªagentçš„è§‚æµ‹ç©ºé—´
                self.envs.action_space[0], # å•ä¸ªagentçš„åŠ¨ä½œç©ºé—´
                device=self.device,
            )
            # å› ä¸ºå…±äº«å‚æ•°ï¼Œæ‰€ä»¥self.actoråˆ—è¡¨ä¸­åªæœ‰ä¸€ä¸ªactorï¼Œå³æ‰€æœ‰agentå…±ç”¨ä¸€å¥—actorç½‘ç»œ
            self.actor.append(agent)

            # å› ä¸ºå…±äº«å‚æ•°ï¼Œagentä¹‹é—´çš„è§‚æµ‹ç©ºé—´å’ŒåŠ¨ä½œç©ºé—´éƒ½è¦åŒæž„
            for agent_id in range(1, self.num_agents):
                # æ‰€ä»¥self.envs.observation_spaceä½œä¸ºa list of obs space for each agentåº”è¯¥ä¿æŒä¸€è‡´
                assert (
                    self.envs.observation_space[agent_id]
                    == self.envs.observation_space[0]
                ), "Agents have heterogeneous observation spaces, parameter sharing is not valid."
                # æ‰€ä»¥self.envs.action_space list of act space for each agentåº”è¯¥ä¿æŒä¸€è‡´
                assert (
                    self.envs.action_space[agent_id] == self.envs.action_space[0]
                ), "Agents have heterogeneous action spaces, parameter sharing is not valid."
                self.actor.append(self.actor[0])
                # self.actoræ˜¯ä¸€ä¸ªlistï¼Œé‡Œé¢æœ‰Nä¸ªä¸€æ¨¡ä¸€æ ·çš„actorï¼Œ

        # actorä¸å…±äº«å‚æ•°
        else:
            self.actor = []
            for agent_id in range(self.num_agents):
                # ç»™æ¯ä¸€ä¸ªagentåˆå§‹åŒ–actorç½‘ç»œï¼Œè¿›å…¥mappo.py ã€æ ¹æ®å…¶ä¸åŒçš„obs_dimå’Œact_dimã€‘
                agent = ALGO_REGISTRY[args["algo"]](
                    {**algo_args["model"], **algo_args["algo"], **algo_args["train"], **env_args},
                    self.envs.observation_space[agent_id],
                    self.envs.action_space[agent_id],
                    device=self.device,
                )
                # å› ä¸ºä¸å…±äº«å‚æ•°ï¼Œæ‰€ä»¥self.actoråˆ—è¡¨ä¸­æœ‰Nä¸ªactorï¼Œæ‰€æœ‰agentæ¯äººä¸€å¥—actorç½‘ç»œ
                self.actor.append(agent)

        # è®­ç»ƒ
        if self.algo_args["render"]["use_render"] is False:  # train, not render
            self.actor_buffer = []
            # ç»™æ¯ä¸€ä¸ªagentåˆ›ç«‹bufferï¼Œåˆå§‹åŒ–bufferï¼Œè¿›å…¥OnPolicyActorBuffer
            for agent_id in range(self.num_agents):
                ac_bu = OnPolicyActorBuffer(
                    # yamlé‡Œmodelå’Œalgoçš„configæ‰“åŒ…ä½œä¸ºargsè¿›å…¥OnPolicyActorBuffer
                    {**algo_args["train"], **algo_args["model"], **env_args},
                    # ã€æ ¹æ®å…¶ä¸åŒçš„obs_dimå’Œact_dimã€‘
                    self.envs.observation_space[agent_id],
                    self.envs.action_space[agent_id],
                )
                # self.actor_bufferåˆ—è¡¨ä¸­æœ‰Nä¸ªbufferï¼Œæ‰€æœ‰agentæ¯äººä¸€å¥—buffer
                self.actor_buffer.append(ac_bu)

            # å•ä¸ªagentçš„share obs space eg: Box(-inf, inf, (54,), float32)
            share_observation_space = self.envs.share_observation_space[0]

            # åˆ›å»ºcentralized criticç½‘ç»œ
            self.critic = VCritic(
                # yamlé‡Œmodelå’Œalgoçš„configæ‰“åŒ…ä½œä¸ºargsè¿›å…¥VCritic
                {**algo_args["model"], **algo_args["algo"], **algo_args["train"], **env_args},
                # ä¸­å¿ƒå¼çš„å€¼å‡½æ•°centralized criticçš„è¾“å…¥æ˜¯å•ä¸ªagentæ‹¿åˆ°çš„share_observation_space dim
                share_observation_space,
                device=self.device,
            )

            # åˆ›å»ºcentralized criticç½‘ç»œçš„bufferï¼ˆ1ä¸ªï¼‰
            # MAPPO trick: åŽŸå§‹è®ºæ–‡section 5.2
            if self.state_type == "EP":
                # EP stands for Environment Provided, as phrased by MAPPO paper.
                # In EP, the global states for all agents are the same.
                # EPçš„å…¨å±€çŠ¶æ€æ˜¯æ‰€æœ‰agentçš„çŠ¶æ€çš„æ‹¼æŽ¥ï¼Œæ‰€ä»¥æ‰€æœ‰agentçš„share_observation_space dimæ˜¯ä¸€æ ·çš„
                self.critic_buffer = OnPolicyCriticBufferEP(
                    {**algo_args["train"], **algo_args["model"], **algo_args["algo"]},
                    share_observation_space,
                )
            elif self.state_type == "FP":
                # FP stands for Feature Pruned, as phrased by MAPPO paper.
                # In FP, the global states for all agents are different, and thus needs the dimension of the number of agents.

                # FPçš„å…¨å±€çŠ¶æ€æ˜¯EP+INDçš„pruneç‰ˆæœ¬ï¼ˆåŒ…å«å…¨å±€çŠ¶æ€ï¼Œagent specificçŠ¶æ€ï¼Œå¹¶ä¸”åˆ é™¤äº†å†—ä½™çŠ¶æ€ï¼‰ï¼Œå› æ­¤æ¯ä¸ªagentä¸ä¸€æ · #TODOï¼šè¿˜æ²¡çœ‹
                self.critic_buffer = OnPolicyCriticBufferFP(
                    {**algo_args["train"], **algo_args["model"], **algo_args["algo"]},
                    share_observation_space,
                    self.num_agents,
                )
            else:
                # TODOï¼š æˆ–è®¸EP+INDçš„éžpruneç‰ˆæœ¬ï¼Ÿ
                raise NotImplementedError

            # MAPPO trick: åŽŸå§‹è®ºæ–‡ section 5.1 - PopArtï¼Ÿ
            if self.algo_args["train"]["use_valuenorm"] is True:
                self.value_normalizer = ValueNorm(1, device=self.device)
            else:
                self.value_normalizer = None

            # çŽ¯å¢ƒçš„logger
            self.logger = LOGGER_REGISTRY[args["env"]](
                args, algo_args, env_args, self.num_agents, self.writter, self.run_dir
            )
            # # Multi-objective monitoring setup
            # try:
            #     from harl.utils.multi_objective_monitor import create_monitor
            #     self.multi_monitor = create_monitor(self.writter, monitor_freq=5, save_dir=self.run_dir)
            # except Exception as e:
            #     print(f"âš ï¸ Multi-objective monitoring not available: {e}")
            #     self.multi_monitor = None

            # Add runner reference to logger
            if hasattr(self, 'logger'):
                self.logger._runner_ref = self

            # # Initialize gradient monitor (only in training mode)
            # monitor_frequency = env_args.get("gradient_monitor_frequency", 5)
            # # self.gradient_monitor = GradientMonitor(self.writter, monitor_frequency)
            # print("âœ… Gradient monitoring system initialized")

        # å¯ä»¥restoreä¹‹å‰è®­ç»ƒåˆ°ä¸€åŠçš„æ¨¡åž‹ç»§ç»­è®­ç»ƒ
        if self.algo_args["train"]["model_dir"] is not None:  # restore model
            self.restore()

    def run(self):
        """Run the training (or rendering) pipeline."""

        # render,ä¸æ˜¯è®­ç»ƒ
        if self.algo_args["render"]["use_render"] is True:
            self.render()
            return

        # å¼€å§‹è®­ç»ƒ
        print("start running")

        # åœ¨çŽ¯å¢ƒresetä¹‹åŽè¿”å›žçš„obsï¼Œshare_obsï¼Œavailable_actionså­˜å…¥æ¯ä¸€ä¸ªactorçš„replay buffer ä»¥åŠ é›†ä¸­å¼criticçš„replay buffer
        self.warmup()

        # è®¡ç®—æ€»å…±éœ€è¦è·‘å¤šå°‘ä¸ªepisode = æ€»è®­ç»ƒæ—¶é—´æ­¥æ•° / æ¯ä¸ªepisodeçš„æ—¶é—´æ­¥æ•° / å¹¶è¡Œçš„çŽ¯å¢ƒæ•° (int)
        episodes = (
                # è®­ç»ƒæ€»æ—¶é—´æ­¥æ•° / æ¯ä¸ªepisodeçš„æ—¶é—´æ­¥æ•° / å¹¶è¡Œçš„çŽ¯å¢ƒæ•°
                int(self.algo_args["train"]["num_env_steps"])
                // self.algo_args["train"]["episode_length"]
                // self.algo_args["train"]["n_rollout_threads"]
        )

        # åˆå§‹åŒ–logger
        self.logger.init(episodes)  # logger callback at the beginning of training

        # å¼€å§‹è®­ç»ƒï¼ï¼ï¼ï¼ï¼ï¼
        # å¯¹äºŽæ¯ä¸€ä¸ªepisode
        for episode in range(1, episodes + 1):
            # start = time.time()
            # å­¦ä¹ çŽ‡æ˜¯å¦éšç€episodeçº¿æ€§é€’å‡
            if self.algo_args["train"]["use_linear_lr_decay"]:
                # æ˜¯å¦å…±äº«actorç½‘ç»œ
                if self.share_param:
                    # åœ¨mappoç»§æ‰¿çš„OnPolicyBaseç±»ä¸­ï¼Œepisodeæ˜¯å½“å‰episodeçš„indexï¼Œepisodesæ˜¯æ€»å…±éœ€è¦è·‘å¤šå°‘ä¸ªepisode
                    self.actor[0].lr_decay(episode, episodes)
                else:
                    for agent_id in range(self.num_agents):
                        self.actor[agent_id].lr_decay(episode, episodes)
                # criticçš„lr_decayå‡½æ•°åœ¨VCriticç±»ä¸­ï¼Œepisodeæ˜¯å½“å‰episodeçš„indexï¼Œepisodesæ˜¯æ€»å…±éœ€è¦è·‘å¤šå°‘ä¸ªepisode
                self.critic.lr_decay(episode, episodes)

            # æ¯ä¸ªepisodeå¼€å§‹çš„æ—¶å€™æ›´æ–°loggeré‡Œé¢çš„episode index
            self.logger.episode_init(
                episode
            )  # logger callback at the beginning of each episode

            # æŠŠactorå’Œcriticç½‘ç»œéƒ½åˆ‡æ¢åˆ°evalæ¨¡å¼
            self.prep_rollout()  # change to eval mode

            # å¯¹äºŽæ‰€æœ‰å¹¶è¡ŒçŽ¯å¢ƒä¸€ä¸ªepisodeçš„æ¯ä¸€ä¸ªæ—¶é—´æ­¥
            for step in range(self.algo_args["train"]["episode_length"]):
                """
                é‡‡æ ·åŠ¨ä½œ - è¿›å…¥actor network 
                values: (n_threads, 1) - æ‰€æœ‰å¹¶è¡ŒçŽ¯å¢ƒåœ¨è¿™ä¸€ä¸ªtimestepçš„criticç½‘ç»œçš„è¾“å‡º
                actions: (n_threads, n_agents, 1) 
                action_log_probs: (n_threads, n_agents, 1)
                rnn_states: (è¿›ç¨‹æ•°é‡, n_agents, rnnå±‚æ•°, rnn_hidden_dim)
                rnn_states_critic: (n_threads, rnnå±‚æ•°, rnn_hidden_dim)
                """
                (
                    values,
                    actions,
                    action_log_probs,
                    rnn_states,  # rnn_statesæ˜¯actorçš„rnnçš„hidden state
                    rnn_states_critic,  # rnn_states_criticæ˜¯criticçš„rnnçš„hidden state
                    action_losss,
                    reward_weights,
                ) = self.collect(step)

                """
                åœ¨å¾—åˆ°åŠ¨ä½œåŽï¼Œæ‰§è¡ŒåŠ¨ä½œ - è¿›å…¥çŽ¯å¢ƒ ShareVecEnv | step
                ä¸ŽçŽ¯å¢ƒäº¤äº’ä¸€ä¸ªstepï¼Œå¾—åˆ°obsï¼Œshare_obsï¼Œrewardsï¼Œdonesï¼Œinfosï¼Œavailable_actions
                # obs: (n_threads, n_agents, obs_dim)
                # share_obs: (n_threads, n_agents, share_obs_dim)
                # rewards: (n_threads, n_agents, 1)
                # dones: (n_threads, n_agents)
                # infos: (n_threads)
                # available_actions: (n_threads, ) of None or (n_threads, n_agents, action_number)
                """
                (
                    obs,
                    share_obs,
                    rewards,
                    mean_v,
                    mean_acc,
                    rewards_safety,
                    rewards_stability,
                    rewards_efficiency,
                    rewards_comfort,
                    safety_SI,
                    efficiency_ASR,
                    efficiency_TFR,
                    stability_SSI,
                    comfort_AI,
                    comfort_JI,
                    control_efficiency,
                    control_reward,
                    comfort_cost,
                    comfort_reward,
                    dones,
                    infos,
                    available_actions,
                    classified_scene_mark,
                ) = self.envs.step(actions)
                """æ¯ä¸ªstepæ›´æ–°loggeré‡Œé¢çš„per_step data"""
                # obs: (n_threads, n_agents, obs_dim)
                # share_obs: (n_threads, n_agents, share_obs_dim)
                # rewards: (n_threads, n_agents, 1)
                # dones: (n_threads, n_agents)
                # infos: (n_threads)
                # available_actions: (n_threads, ) of None or (n_threads, n_agents, action_number)
                data = (
                    obs,
                    share_obs,
                    rewards,
                    dones,
                    infos,
                    available_actions,
                    values,
                    actions,
                    action_log_probs,
                    rnn_states,
                    rnn_states_critic,
                    action_losss,
                    mean_v,
                    mean_acc,
                    rewards_safety,
                    rewards_stability,
                    rewards_efficiency,
                    rewards_comfort,
                    reward_weights,
                    safety_SI,
                    efficiency_ASR,
                    efficiency_TFR,
                    stability_SSI,
                    comfort_AI,
                    comfort_JI,
                    control_efficiency,
                    control_reward,
                    comfort_cost,
                    comfort_reward,
                    classified_scene_mark,
                )

                self.logger.per_step(data)  # logger callback at each step

                """æŠŠè¿™ä¸€æ­¥çš„æ•°æ®å­˜å…¥æ¯ä¸€ä¸ªactorçš„replay buffer ä»¥åŠ é›†ä¸­å¼criticçš„replay buffer"""
                self.insert(data)  # insert data into buffer

            # æ”¶é›†å®Œäº†ä¸€ä¸ªepisodeçš„æ‰€æœ‰timestep dataï¼Œå¼€å§‹è®¡ç®—returnï¼Œæ›´æ–°ç½‘ç»œ
            # compute Q and V using GAE or not
            self.compute()

            # ç»“æŸè¿™ä¸€ä¸ªepisodeçš„äº¤äº’æ•°æ®æ”¶é›†
            # æŠŠactorå’Œcriticç½‘ç»œéƒ½åˆ‡æ¢å›žtrainæ¨¡å¼
            self.prep_training()

            # ä»Žè¿™é‡Œå¼€å§‹ï¼Œmappoå’Œhappoä¸ä¸€æ ·äº†
            actor_train_infos, critic_train_info = self.train()

            # è®¾ç½®timestepä¿¡æ¯ä¾›ç›‘æŽ§ä½¿ç”¨
            self.total_num_steps = (
                    episode * self.algo_args["train"]["episode_length"] *
                    self.algo_args["train"]["n_rollout_threads"]
            )

            # # ä¼ é€’ç›‘æŽ§å™¨å’Œtimestepä¿¡æ¯
            # if hasattr(self, 'multi_monitor'):
            #     # æ›´æ–°ç›‘æŽ§å™¨çš„timestepä¿¡æ¯
            #     if self.multi_monitor and hasattr(self.multi_monitor, 'update'):
            #         self.multi_monitor._current_timestep = self.total_num_steps
            #
            # # è®¾ç½®episodeå±žæ€§
            # self.episode = episode
            # # Monitor gradients and parameters
            # try:
            #     self.monitor_gradients_and_parameters()
            #
            #     # å¢žåŠ ç›‘æŽ§è®¡æ•°
            #     if hasattr(self, 'gradient_monitor'):
            #         self.gradient_monitor.increment_update_count()
            #
            # except Exception as e:
            #     print(f"âš ï¸ Monitoring failed for episode {episode}: {e}")
            #     # è®­ç»ƒç»§ç»­ï¼Œç›‘æŽ§å¤±è´¥ä¸å½±å“è®­ç»ƒæµç¨‹

            # log information
            if episode % self.algo_args["train"]["log_interval"] == 0:
                save_model_signal, current_timestep = self.logger.episode_log(
                                    actor_train_infos,
                                    critic_train_info,
                                    self.actor_buffer,
                                    self.critic_buffer,
                                    self.env_args["save_collision"],
                                    self.env_args["save_episode_mean_speed"],
                                )
                # # ðŸ”§ è®°å½•ç›‘æŽ§ç³»ç»ŸçŠ¶æ€
                # if hasattr(self, 'gradient_monitor'):
                #     try:
                #         # è¾“å‡ºç›‘æŽ§æ‘˜è¦
                #         mismatch_report = self.gradient_monitor.get_dimension_mismatch_report()
                #         if mismatch_report['mismatch_count'] > 0:
                #             print(f"ðŸ“Š Monitoring Summary - Episode {episode}:")
                #             print(f"   - Dimension mismatches: {mismatch_report['mismatch_count']} parameters")
                #             print(
                #                 f"   - Mismatched params: {', '.join(mismatch_report['mismatch_params'][:3])}{'...' if len(mismatch_report['mismatch_params']) > 3 else ''}")
                #             print("   â„¹ï¸  This is normal in MAPPO due to different obs/state dimensions")
                #     except Exception as e:
                #         print(f"âš ï¸ Error generating monitoring summary: {e}")
                if save_model_signal:
                    self.save_good_model(current_timestep)
                else:
                    pass

            # eval
            if episode % self.algo_args["train"]["eval_interval"] == 0:
                if self.algo_args["eval"]["use_eval"]:
                    self.prep_rollout()
                    self.eval()
                self.save()

            # æŠŠä¸Šä¸€ä¸ªepisodeäº§ç”Ÿçš„æœ€åŽä¸€ä¸ªtimestepçš„stateæ”¾å…¥bufferçš„æ–°çš„episodeçš„ç¬¬ä¸€ä¸ªtimestep
            self.after_update()
            # end = time.time()
            # print(f"Episode {episode} takes {end - start} seconds")

    def warmup(self):
        """
        Warm up the replay buffer.
        åœ¨çŽ¯å¢ƒresetä¹‹åŽè¿”å›žçš„obsï¼Œshare_obsï¼Œavailable_actionså­˜å…¥æ¯ä¸€ä¸ªactorçš„replay buffer ä»¥åŠ é›†ä¸­å¼criticçš„replay buffer
        """
        """
        resetæ‰€æœ‰çš„å¹¶è¡ŒçŽ¯å¢ƒï¼Œè¿”å›ž
        obs: (n_threads, n_agents, obs_dim)
        share_obs: (n_threads, n_agents, share_obs_dim)
        available_actions: (n_threads, n_agents, action_dim)
        """
        obs, share_obs, available_actions, classified_scene_mark = self.envs.reset()

        # å‡†å¤‡é˜¶æ®µ---æ¯ä¸€ä¸ªactorçš„replay buffer
        for agent_id in range(self.num_agents):
            # self.actor_buffer[agent_id].obsæ˜¯[episode_length+1, è¿›ç¨‹æ•°é‡, obs_shape]
            # self.actor_buffer[agent_id].obs[0]æ˜¯episodeåœ¨t=0æ—¶çš„obs [è¿›ç¨‹æ•°é‡, obs_shape]
            # æ›´å¤šç»†èŠ‚çœ‹OnPolicyActorBuffer
            # åœ¨çŽ¯å¢ƒresetä¹‹åŽï¼ŒæŠŠæ‰€æœ‰å¹¶è¡ŒçŽ¯å¢ƒä¸‹ä¸“å±žäºŽagent_idçš„obsæ”¾å…¥ä¸“å±žäºŽagent_idçš„bufferçš„self.obsçš„ç¬¬ä¸€æ­¥é‡Œ
            self.actor_buffer[agent_id].obs[0] = obs[:, agent_id].copy()

            if self.actor_buffer[agent_id].available_actions is not None:
                # åœ¨çŽ¯å¢ƒresetä¹‹åŽ
                # æŠŠæ‰€æœ‰å¹¶è¡ŒçŽ¯å¢ƒä¸‹çš„ä¸“å±žäºŽagent_idçš„available_actionsæ”¾å…¥ä¸“å±žäºŽagent_idçš„bufferçš„self.available_actionsçš„ç¬¬ä¸€æ­¥é‡Œ
                self.actor_buffer[agent_id].available_actions[0] = available_actions[:, agent_id].copy()

        # å‡†å¤‡é˜¶æ®µ---é›†ä¸­å¼criticçš„replay buffer
        # æ›´å¤šç»†èŠ‚çœ‹OnPolicyCriticBufferEP/FP
        if self.state_type == "EP":
            # åœ¨çŽ¯å¢ƒresetä¹‹åŽ
            # æŠŠæ‰€æœ‰å¹¶è¡ŒçŽ¯å¢ƒä¸‹çš„ä¸“å±žäºŽagent_idçš„share_obsæ”¾å…¥ä¸“å±žäºŽagent_idçš„bufferçš„self.share_obsçš„ç¬¬ä¸€æ­¥é‡Œ
            self.critic_buffer.share_obs[0] = share_obs[:, 0].copy()
        elif self.state_type == "FP":
            self.critic_buffer.share_obs[0] = share_obs.copy()

    @torch.no_grad()  # å‰å‘ï¼Œæ²¡æœ‰åå‘ä¼ æ’­ï¼Œä¸éœ€è¦è®¡ç®—æ¢¯åº¦
    def collect(self, step):
        """
        Collect actions and values from actors and critics.
        ä»Žactorå’Œcriticä¸­æ”¶é›†actionså’Œvalues
        Args:
            step: step in the episode. è¿™ä¸€ä¸ªepisodeçš„ç¬¬å‡ æ­¥
        Returns:
            values, actions, action_log_probs, rnn_states, rnn_states_critic
            è¾“å‡ºvalues, actions, action_log_probs, rnn_statesï¼ˆactorï¼‰, rnn_states_critic
        """

        # ä»Žnä¸ªactorä¸­æ”¶é›†actions, action_log_probs, rnn_states
        action_collector = []
        action_log_prob_collector = []
        rnn_state_collector = []
        action_loss_collector = []
        reward_weights_collector = []

        # ä»Žcriticä¸­æ”¶é›†values, rnn_states_critic
        values = []
        rnn_states_critic = []

        """
        é¦–å…ˆæ˜¯actorçš„æ”¶é›† - ä¼ªä»£ç 12-13è¡Œ
        # å¯¹äºŽæ¯ä¸€ä¸ªagentå¯¹åº”çš„self.actor[agent_id]
        # ç»™actor[agent_id]è¾“å…¥:  (æœ‰å…³è¾“å…¥å¯ä»¥å‚è€ƒOnPolicyActorBufferçš„åˆå§‹åŒ–)
            - å½“å‰æ—¶åˆ»obs
            - ä¸Šä¸€æ—¶åˆ»è¾“å‡ºçš„çš„rnn_state
            - mask (done or not)
            - å½“å‰æ™ºèƒ½ä½“çš„å¯ç”¨åŠ¨ä½œ
            - bool(æœ‰æ²¡æœ‰available_actions)
        # è¾“å‡º:
            - action
            - action_log_prob
            - rnn_state(actor)
        """
        # å¯¹äºŽæ¯ä¸€ä¸ªagentæ¥è¯´
        for agent_id in range(self.num_agents):
            # self.actor[agent_id].get_actionså‚è€ƒOnPolicyBase
            # actions: (torch.Tensor) actions for the given inputs. ã€thread_num, 1ã€‘
            # action_log_probs: (torch.Tensor) log probabilities of actions. ã€thread_num, 1ã€‘
            # rnn_states_actor: (torch.Tensor) updated RNN states for actor. ã€thread_num, rnnå±‚æ•°ï¼Œrnn_state_dimã€‘
            if step == 0:
                prew_weights = self.actor_buffer[agent_id].rewards_weights[0]
            else:
                prew_weights = self.actor_buffer[agent_id].rewards_weights[step - 1]
            action, action_log_prob, rnn_state, action_loss, reward_weight = self.actor[agent_id].get_actions(
                self.actor_buffer[agent_id].obs[step],
                self.actor_buffer[agent_id].rnn_states[step],
                prew_weights,
                self.actor_buffer[agent_id].masks[step],
                self.actor_buffer[agent_id].available_actions[step]
                if self.actor_buffer[agent_id].available_actions is not None
                else None,
            )  # TODO: æ£€æŸ¥rewards_weights[step] è¿˜æ˜¯ rewards_weights[step-1]
            # tensorè½¬numpy
            action_collector.append(_t2n(action))
            action_log_prob_collector.append(_t2n(action_log_prob))
            rnn_state_collector.append(_t2n(rnn_state))
            action_loss_collector.append(_t2n(action_loss))
            reward_weights_collector.append(_t2n(reward_weight))

        # è½¬ç½® (n_agents, n_threads, dim) -> (n_threads, n_agents, dim)
        actions = np.array(action_collector).transpose(1, 0, 2)
        action_log_probs = np.array(action_log_prob_collector).transpose(1, 0, 2)
        rnn_states = np.array(rnn_state_collector).transpose(1, 0, 2, 3)
        action_losss = np.array(action_loss_collector).transpose(1, 0, 2)
        reward_weights = np.array(reward_weights_collector).transpose(1, 0, 2)

        """
        ç„¶åŽæ˜¯criticçš„æ”¶é›† - ä¼ªä»£ç 14è¡Œ
        ä¸¤ç§æƒ…å†µï¼šcriticçš„è¾“å…¥æ˜¯æ‰€æœ‰agent obsçš„concateèµ·æ¥(EP)è¿˜æ˜¯ç»è¿‡å¤„ç†(FP)
        # ç»™criticè¾“å…¥:
            - å½“å‰æ—¶åˆ»çš„share_obs
            - ä¸Šä¸€æ—¶åˆ»çš„rnn_state_critic
            - mask
        # è¾“å‡º:
            - value
            - rnn_state_critic
        """
        # collect values, rnn_states_critic from 1 critic
        # å‚è€ƒv_critics.py
        if self.state_type == "EP":
            value, rnn_state_critic = self.critic.get_values(
                self.critic_buffer.share_obs[step],
                self.critic_buffer.rnn_states_critic[step],
                self.critic_buffer.masks[step],
            )
            # (n_threads, dim)

            # tensorè½¬numpy
            values = _t2n(value)
            rnn_states_critic = _t2n(rnn_state_critic)

        elif self.state_type == "FP":
            value, rnn_state_critic = self.critic.get_values(
                np.concatenate(self.critic_buffer.share_obs[step]),
                np.concatenate(self.critic_buffer.rnn_states_critic[step]),
                np.concatenate(self.critic_buffer.masks[step]),
            )  # concatenate (n_threads, n_agents, dim) into (n_threads * n_agents, dim)
            # split (n_threads * n_agents, dim) into (n_threads, n_agents, dim)
            values = np.array(
                np.split(_t2n(value), self.algo_args["train"]["n_rollout_threads"])
            )
            rnn_states_critic = np.array(
                np.split(
                    _t2n(rnn_state_critic), self.algo_args["train"]["n_rollout_threads"]
                )
            )

        return values, actions, action_log_probs, rnn_states, rnn_states_critic, action_losss, reward_weights

    def insert(self, data):
        """æŠŠè¿™ä¸€ä¸ªtime stepçš„æ•°æ®æ’å…¥åˆ°bufferä¸­"""
        (
            obs,  # (n_threads, n_agents, obs_dim)
            share_obs,  # (n_threads, n_agents, share_obs_dim)
            rewards,  # (n_threads, n_agents, 1)
            dones,  # (n_threads, n_agents)
            infos,  # tuple of list of Dict, shape: (n_threads, n_agents, 4)
            available_actions,  # (n_threads, ) of None or (n_threads, n_agents, action_dim)
            values,  # EP: (n_threads, 1), FP: (n_threads, n_agents, 1)
            actions,  # (n_threads, n_agents, 1)
            action_log_probs,  # (n_threads, n_agents, 1)
            rnn_states,  # (n_threads, n_agents, rnnå±‚æ•°, hidden_dim)
            rnn_states_critic,  # EP: (n_threads, rnnå±‚æ•°, hidden_dim), FP: (n_threads, n_agents, dim)
            action_losss,  # (n_threads, n_agents, 1)
            mean_v,  # (n_threads, n_agents, 1)
            mean_acc,  # (n_threads, n_agents, 1)
            rewards_safety,
            rewards_stability,
            rewards_efficiency,
            rewards_comfort,
            reward_weights,
            safety_SI,
            efficiency_ASR,
            efficiency_TFR,
            stability_SSI,
            comfort_AI,
            comfort_JIs,
            control_efficiencys,
            control_reward,
            comfort_cost,
            comfort_reward,
            classified_scene_mark,
        ) = data

        # æ£€æŸ¥æ‰€æœ‰env threadæ˜¯å¦done (n_threads, )
        dones_env = np.all(dones, axis=1)

        """
        é‡ç½®actorå’Œcriticçš„rnn_state
        rnn_states: (n_threads, n_agents, rnnå±‚æ•°, hidden_dim)
        rnn_states_critic: (n_threads, rnnå±‚æ•°, hidden_dim)
        """
        # å¦‚æžœå“ªä¸ªenv doneäº†ï¼Œé‚£ä¹ˆå°±æŠŠé‚£ä¸ªçŽ¯å¢ƒçš„rnn_state (æ‰€æœ‰actor)ç½®ä¸º0
        rnn_states[
            dones_env == True
        ] = np.zeros(  # if env is done, then reset rnn_state to all zero
            (
                (dones_env == True).sum(), #dones_envé‡Œæœ‰å‡ ä¸ªtrueï¼Œå‡ ä¸ªå¹¶è¡ŒçŽ¯å¢ƒdoneäº†
                self.num_agents,
                self.recurrent_n,
                self.rnn_hidden_size,
            ),
            dtype=np.float32,
        )

        # If env is done, then reset rnn_state_critic to all zero
        # å¦‚æžœå“ªä¸ªenv doneäº†ï¼Œé‚£ä¹ˆå°±æŠŠé‚£ä¸ªçŽ¯å¢ƒçš„rnn_state (critic)ç½®ä¸º0
        if self.state_type == "EP":
            rnn_states_critic[dones_env == True] = np.zeros(
                ((dones_env == True).sum(), self.recurrent_n, self.rnn_hidden_size),
                dtype=np.float32,
            )
        elif self.state_type == "FP":
            rnn_states_critic[dones_env == True] = np.zeros(
                (
                    (dones_env == True).sum(),
                    self.num_agents,
                    self.recurrent_n,
                    self.rnn_hidden_size,
                ),
                dtype=np.float32,
            )

        """
        é‡ç½®masks
        æŠŠå·²ç»doneäº†çš„envçš„maskç”±1ç½®ä¸º0
        è¿™ä¸ªmaskæ˜¯è¡¨ç¤ºç€ä»€ä¹ˆæ—¶å€™å“ªä¸€ä¸ªå¹¶è¡ŒçŽ¯å¢ƒçš„rnn_stateè¦é‡ç½®
        # masks use 0 to mask out threads that just finish.
        # this is used for denoting at which point should rnn state be reset
        array of shape (n_rollout_threads, n_agents, 1)
        """
        # åˆå§‹åŒ–æ‰€æœ‰çŽ¯å¢ƒçš„maskæ˜¯1
        masks = np.ones(
            (self.algo_args["train"]["n_rollout_threads"], self.num_agents, 1),
            dtype=np.float32,
        )
        # å¦‚æžœå“ªä¸ªenv doneäº†ï¼Œé‚£ä¹ˆå°±æŠŠé‚£ä¸ªçŽ¯å¢ƒçš„maskç½®ä¸º0
        masks[dones_env == True] = np.zeros(
            ((dones_env == True).sum(), self.num_agents, 1), dtype=np.float32
        )
        """
        é‡ç½®active_masks
        æŠŠå·²ç»æ­»æŽ‰çš„agentçš„maskç”±1ç½®ä¸º0
        # active_masks use 0 to mask out agents that have died
        array of shape (n_rollout_threads, n_agents, 1)
        """
        # åˆå§‹åŒ–æ‰€æœ‰çŽ¯å¢ƒçš„maskæ˜¯1
        active_masks = np.ones(
            (self.algo_args["train"]["n_rollout_threads"], self.num_agents, 1),
            dtype=np.float32,
        )
        # å¦‚æžœå“ªä¸ªagent doneäº†ï¼Œé‚£ä¹ˆå°±æŠŠé‚£ä¸ªagentçš„maskç½®ä¸º0
        active_masks[dones == True] = np.zeros(
            ((dones == True).sum(), 1), dtype=np.float32
        )
        # å¦‚æžœå“ªä¸ªenv doneäº†ï¼Œé‚£ä¹ˆå°±æŠŠé‚£ä¸ªçŽ¯å¢ƒçš„maskç½®ä¸º1
        active_masks[dones_env == True] = np.ones(
            ((dones_env == True).sum(), self.num_agents, 1), dtype=np.float32
        )

        """
        é‡ç½®bad_masks
        array of shape (n_rollout_threads, 1)
        """
        # bad_masks use 0 to denote truncation and 1 to denote termination
        if self.state_type == "EP":
            bad_masks = np.array(
                [
                    [0.0]
                    if "bad_transition" in info[0].keys()
                    and info[0]["bad_transition"] == True
                    else [1.0]
                    for info in infos
                ]
            )
        elif self.state_type == "FP":
            bad_masks = np.array(
                [
                    [
                        [0.0]
                        if "bad_transition" in info[agent_id].keys()
                        and info[agent_id]["bad_transition"] == True
                        else [1.0]
                        for agent_id in range(self.num_agents)
                    ]
                    for info in infos
                ]
            )

        # æ’å…¥actor_buffer
        dynamic_rewards = np.zeros_like(rewards_comfort)
        # æŠŠæ‰€æœ‰å¥–åŠ±åˆå¹¶æˆä¸€ä¸ªå¼ é‡ï¼Œå¤§å°ä¸º (20, 4, 4)
        all_rewards = np.concatenate([
            rewards_safety,  # å¯¹åº” reward_weights çš„ç¬¬ 0 åˆ—
            rewards_efficiency,  # å¯¹åº” reward_weights çš„ç¬¬ 1 åˆ—
            rewards_stability,  # å¯¹åº” reward_weights çš„ç¬¬ 2 åˆ—
            rewards_comfort  # å¯¹åº” reward_weights çš„ç¬¬ 3 åˆ—
        ], axis=2)

        for agent_id in range(self.num_agents):
            self.actor_buffer[agent_id].insert(
                obs[:, agent_id],
                rnn_states[:, agent_id],
                actions[:, agent_id],
                action_log_probs[:, agent_id],
                action_losss[:, agent_id],
                mean_v[:],
                mean_acc[:],
                reward_weights[:, agent_id],
                rewards_safety[:, agent_id],
                rewards_stability[:, agent_id],
                rewards_efficiency[:, agent_id],
                rewards_comfort[:, agent_id],
                safety_SI[:],
                efficiency_ASR[:],
                efficiency_TFR[:],
                stability_SSI[:],
                comfort_AI[:],
                comfort_JIs[:],
                control_efficiencys[:],
                control_reward[:],
                comfort_cost[:],
                comfort_reward[:],
                masks[:, agent_id],
                active_masks[:, agent_id],
                available_actions[:, agent_id]
                if available_actions[0] is not None
                else None,
            )
            dynamic_rewards[:, agent_id, 0] = np.sum(
                reward_weights[:, agent_id, :] * all_rewards[:, agent_id, :],
                axis=1
            ).squeeze()

            # æ’å…¥critic_buffer
        if self.state_type == "EP":
            min_rewards = np.min(rewards, axis=1)
            mean_rewards = np.mean(rewards, axis=1)
            min_dynamic_rewards = np.min(dynamic_rewards, axis=1)
            mean_dynamic_rewards = np.mean(dynamic_rewards, axis=1)
            self.critic_buffer.insert(
                share_obs[:, 0],
                rnn_states_critic,
                values,
                mean_rewards, # rewards[:, 0],   #TODOï¼šè¿™é‡Œçš„rewardså¯ä»¥æ ¹æ®å…·ä½“éœ€æ±‚è°ƒæ•´
                min_rewards,
                masks[:, 0],
                bad_masks,
            )
        elif self.state_type == "FP":
            self.critic_buffer.insert(
                share_obs,
                rnn_states_critic,
                values,
                rewards,
                masks,
                bad_masks
            )

    @torch.no_grad()
    def compute(self):
        """Compute returns and advantages.
        è®­ç»ƒå¼€å§‹ä¹‹å‰ï¼Œé¦–å…ˆè°ƒç”¨self.compute()å‡½æ•°è®¡ç®—è¿™ä¸ªepisodeçš„æŠ˜æ‰£å›žæŠ¥
        åœ¨è®¡ç®—æŠ˜æ‰£å›žæŠ¥ä¹‹å‰ï¼Œå…ˆç®—è¿™ä¸ªepisodeæœ€åŽä¸€ä¸ªçŠ¶æ€çš„çŠ¶æ€å€¼å‡½æ•°next_valuesï¼Œå…¶shape=(çŽ¯å¢ƒæ•°, 1)ç„¶åŽè°ƒç”¨compute_returnså‡½æ•°è®¡ç®—æŠ˜æ‰£å›žæŠ¥
        Compute critic evaluation of the last state, Vï¼ˆs-Tï¼‰
        and then let buffer compute returns, which will be used during training.
        """
        # è®¡ç®—criticçš„æœ€åŽä¸€ä¸ªstateçš„å€¼
        if self.state_type == "EP":
            next_value, _ = self.critic.get_values(
                self.critic_buffer.share_obs[-1],
                self.critic_buffer.rnn_states_critic[-1],
                self.critic_buffer.masks[-1],
            )
            next_value = _t2n(next_value)
        elif self.state_type == "FP":
            next_value, _ = self.critic.get_values(
                np.concatenate(self.critic_buffer.share_obs[-1]),
                np.concatenate(self.critic_buffer.rnn_states_critic[-1]),
                np.concatenate(self.critic_buffer.masks[-1]),
            )
            next_value = np.array(
                np.split(_t2n(next_value), self.algo_args["train"]["n_rollout_threads"])
            )

        # next_value --- np.array shape=(çŽ¯å¢ƒæ•°, 1) -- æœ€åŽä¸€ä¸ªçŠ¶æ€çš„çŠ¶æ€å€¼
        # self.value_normalizer --- ValueNorm
        self.critic_buffer.compute_returns(next_value, self.value_normalizer)

    def train(self):
        """Train the model."""
        raise NotImplementedError

    def after_update(self):
        """Do the necessary data operations after an update.
        After an update, copy the data at the last step to the first position of the buffer.
        This will be used for then generating new actions.
        """
        for agent_id in range(self.num_agents):
            self.actor_buffer[agent_id].after_update()
        self.critic_buffer.after_update()

    @torch.no_grad()
    def eval(self):
        """Evaluate the model."""
        self.logger.eval_init()  # logger callback at the beginning of evaluation
        eval_episode = 0

        eval_obs, eval_share_obs, eval_available_actions, classified_scene_mark = self.eval_envs.reset()

        eval_rnn_states = np.zeros(
            (
                self.algo_args["eval"]["n_eval_rollout_threads"],
                self.num_agents,
                self.recurrent_n,
                self.rnn_hidden_size,
            ),
            dtype=np.float32,
        )
        eval_reward_weights = np.zeros(
            (self.algo_args["eval"]["n_eval_rollout_threads"], self.num_agents, self.num_reward_head),
            dtype=np.float32,
        )
        eval_masks = np.ones(
            (self.algo_args["eval"]["n_eval_rollout_threads"], self.num_agents, 1),
            dtype=np.float32,
        )

        while True:
            eval_actions_collector = []
            for agent_id in range(self.num_agents):
                eval_actions, temp_rnn_state = self.actor[agent_id].act(
                    eval_obs[:, agent_id],
                    eval_rnn_states[:, agent_id],
                    eval_reward_weights[:, agent_id],
                    eval_masks[:, agent_id],
                    eval_available_actions[:, agent_id]
                    if eval_available_actions[0] is not None
                    else None,
                    deterministic=True,
                )
                eval_rnn_states[:, agent_id] = _t2n(temp_rnn_state)
                eval_actions_collector.append(_t2n(eval_actions))

            eval_actions = np.array(eval_actions_collector).transpose(1, 0, 2)

            (
                eval_obs,
                eval_share_obs,
                eval_rewards,
                eval_speed,
                eval_acceleration,
                eval_rewards_safety,
                eval_rewards_stability,
                eval_rewards_efficiency,
                eval_rewards_comfort,
                eval_safety_SI,
                eval_efficiency_ASR,
                eval_efficiency_TFR,
                eval_stability_SSI,
                eval_comfort_AI,
                eval_comfort_JIs,
                eval_control_efficiencys,
                eval_control_reward,
                eval_comfort_cost,
                eval_comfort_reward,
                eval_dones,
                eval_infos,
                eval_available_actions,
                classified_scene_mark,
            ) = self.eval_envs.step(eval_actions)
            eval_data = (
                eval_obs,
                eval_share_obs,
                eval_rewards,
                eval_speed,
                eval_acceleration,
                eval_dones,
                eval_infos,
                eval_available_actions,
            )
            self.logger.eval_per_step(
                eval_data
            )  # logger callback at each step of evaluation

            eval_dones_env = np.all(eval_dones, axis=1)

            eval_rnn_states[
                eval_dones_env == True
            ] = np.zeros(  # if env is done, then reset rnn_state to all zero
                (
                    (eval_dones_env == True).sum(),
                    self.num_agents,
                    self.recurrent_n,
                    self.rnn_hidden_size,
                ),
                dtype=np.float32,
            )

            eval_masks = np.ones(
                (self.algo_args["eval"]["n_eval_rollout_threads"], self.num_agents, 1),
                dtype=np.float32,
            )
            eval_masks[eval_dones_env == True] = np.zeros(
                ((eval_dones_env == True).sum(), self.num_agents, 1), dtype=np.float32
            )

            for eval_i in range(self.algo_args["eval"]["n_eval_rollout_threads"]):
                if eval_dones_env[eval_i]:
                    eval_episode += 1
                    self.logger.eval_thread_done(
                        eval_i
                    )  # logger callback when an episode is done

            if eval_episode >= self.algo_args["eval"]["eval_episodes"]:
                self.logger.eval_log(
                    eval_episode
                )  # logger callback at the end of evaluation
                break

    @torch.no_grad()
    def render(self):
        """Render the model."""
        print("start rendering")
        if self.manual_expand_dims:
            # this env needs manual expansion of the num_of_parallel_envs dimension
            for _ in range(self.algo_args["render"]["render_episodes"]):
                eval_obs, _, eval_available_actions, classified_scene_mark = self.envs.reset()
                eval_obs = np.expand_dims(np.array(eval_obs), axis=0)
                eval_available_actions = (
                    np.expand_dims(np.array(eval_available_actions), axis=0)
                    if eval_available_actions is not None
                    else None
                )
                eval_rnn_states = np.zeros(
                    (
                        self.env_num,
                        self.num_agents,
                        self.recurrent_n,
                        self.rnn_hidden_size,
                    ),
                    dtype=np.float32,
                )
                eval_reward_weights = np.zeros(
                    (self.env_num, self.num_agents, self.num_reward_head),
                    dtype=np.float32,
                )
                eval_masks = np.ones(
                    (self.env_num, self.num_agents, 1), dtype=np.float32
                )
                rewards = 0
                mean_v = 0
                mean_acc = 0
                while True:
                    eval_actions_collector = []
                    for agent_id in range(self.num_agents):
                        eval_actions, temp_rnn_state = self.actor[agent_id].act(
                            eval_obs[:, agent_id],
                            eval_rnn_states[:, agent_id],
                            eval_reward_weights[:, agent_id],
                            eval_masks[:, agent_id],
                            eval_available_actions[:, agent_id]
                            if eval_available_actions is not None
                            else None,
                            deterministic=True,
                        )
                        eval_rnn_states[:, agent_id] = _t2n(temp_rnn_state)
                        eval_actions_collector.append(_t2n(eval_actions))
                    eval_actions = np.array(eval_actions_collector).transpose(1, 0, 2)
                    (
                        eval_obs,
                        _,
                        eval_rewards,
                        eval_mean_v,
                        eval_mean_acc,
                        eval_rewards_safety,
                        eval_rewards_stability,
                        eval_rewards_efficiency,
                        eval_rewards_comfort,
                        eval_safety_SI,
                        eval_efficiency_ASR,
                        eval_efficiency_TFR,
                        eval_stability_SSI,
                        eval_comfort_AI,
                        eval_comfort_JIs,
                        eval_control_efficiencys,
                        eval_control_reward,
                        eval_comfort_cost,
                        eval_comfort_reward,
                        eval_dones,
                        infos,
                        eval_available_actions,
                        classified_scene_marks,
                    ) = self.envs.step(eval_actions[0])
                    # print('Reward for each CAV:', eval_rewards)
                    # print()
                    rewards += eval_rewards[0][0]
                    mean_v += eval_mean_v
                    mean_acc += eval_mean_acc
                    eval_obs = np.expand_dims(np.array(eval_obs), axis=0)
                    eval_available_actions = (
                        np.expand_dims(np.array(eval_available_actions), axis=0)
                        if eval_available_actions is not None
                        else None
                    )
                    if self.manual_render:
                        self.envs.render()
                    if self.manual_delay:
                        time.sleep(0.001)
                    if np.all(eval_dones):
                        print(f"total reward of this episode: {rewards}")
                        print(f"Episode Step Time: {infos[0]['step_time']}")
                        print(f"Episode Mean Speed: {mean_v/(infos[0]['step_time']*10)}")
                        print(f"Episode Mean Acceleration: {mean_acc/(infos[0]['step_time']*10)}")
                        print(f"Collision: {infos[0]['collision']}")
                        print(f"Done Reason: {infos[0]['done_reason']}")
                        print('--------------------------------------')
                        break
        else:
            # this env does not need manual expansion of the num_of_parallel_envs dimension
            # such as dexhands, which instantiates a parallel env of 64 pair of hands
            for _ in range(self.algo_args["render"]["render_episodes"]):
                eval_obs, _, eval_available_actions = self.envs.reset()
                eval_rnn_states = np.zeros(
                    (
                        self.env_num,
                        self.num_agents,
                        self.recurrent_n,
                        self.rnn_hidden_size,
                    ),
                    dtype=np.float32,
                )
                eval_masks = np.ones(
                    (self.env_num, self.num_agents, 1), dtype=np.float32
                )
                rewards = 0
                while True:
                    eval_actions_collector = []
                    for agent_id in range(self.num_agents):
                        eval_actions, temp_rnn_state = self.actor[agent_id].act(
                            eval_obs[:, agent_id],
                            eval_rnn_states[:, agent_id],
                            eval_masks[:, agent_id],
                            eval_available_actions[:, agent_id]
                            if eval_available_actions[0] is not None
                            else None,
                            deterministic=True,
                        )
                        eval_rnn_states[:, agent_id] = _t2n(temp_rnn_state)
                        eval_actions_collector.append(_t2n(eval_actions))
                    eval_actions = np.array(eval_actions_collector).transpose(1, 0, 2)
                    (
                        eval_obs,
                        _,
                        eval_rewards,
                        eval_dones,
                        _,
                        eval_available_actions,
                    ) = self.envs.step(eval_actions)
                    rewards += eval_rewards[0][0][0]
                    if self.manual_render:
                        self.envs.render()
                    if self.manual_delay:
                        time.sleep(0.001)
                    if eval_dones[0][0]:
                        print(f"total reward of this episode: {rewards}")
                        break
        if "smac" in self.args["env"]:  # replay for smac, no rendering
            if "v2" in self.args["env"]:
                self.envs.env.save_replay()
            else:
                self.envs.save_replay()

    def prep_rollout(self):
        """Prepare for rollout.
        æŠŠactorå’Œcriticç½‘ç»œéƒ½åˆ‡æ¢åˆ°evalæ¨¡å¼
        """

        # æ¯ä¸€ä¸ªactor
        for agent_id in range(self.num_agents):
            # æµ‹è¯•actorç½‘ç»œç»“æž„ actor_policy.eval()
            self.actor[agent_id].prep_rollout()

        # é›†ä¸­å¼critic
        # æµ‹è¯•criticç½‘ç»œç»“æž„ critic_policy.eval()
        self.critic.prep_rollout()

    def prep_training(self):
        """Prepare for training.
        æŠŠactorå’Œcriticç½‘ç»œéƒ½åˆ‡æ¢å›žtrainæ¨¡å¼"""
        for agent_id in range(self.num_agents):
            # å¼€å§‹å‡†å¤‡è®­ç»ƒ actor_policy.train()
            self.actor[agent_id].prep_training()
        # å¼€å§‹å‡†å¤‡è®­ç»ƒ critic_policy.train()
        self.critic.prep_training()

    def save(self):
        """Save model parameters."""
        for agent_id in range(self.num_agents):
            policy_actor = self.actor[agent_id].actor
            torch.save(
                policy_actor.state_dict(),
                str(self.save_dir) + "/actor_agent" + str(agent_id) + ".pt",
            )
        policy_critic = self.critic.critic
        torch.save(
            policy_critic.state_dict(), str(self.save_dir) + "/critic_agent" + ".pt"
        )
        if self.value_normalizer is not None:
            torch.save(
                self.value_normalizer.state_dict(),
                str(self.save_dir) + "/value_normalizer" + ".pt",
            )
    def save_good_model(self, current_timestep):
        """Save Model when the model is good."""

        policy_actor = self.actor[0].actor
        save_good_dir = self.save_dir + "/good_model"
        if not os.path.exists(save_good_dir):
            os.mkdir(save_good_dir)
        torch.save(
            policy_actor.state_dict(),
            save_good_dir + "/actor_agent" + str(0) + "_" + str(current_timestep) + ".pt",
        )
        policy_critic = self.critic.critic
        torch.save(
            policy_critic.state_dict(), save_good_dir + "/critic_agent" + "_" + str(current_timestep) +  ".pt"
        )
        if self.value_normalizer is not None:
            torch.save(
                self.value_normalizer.state_dict(),
                save_good_dir + "/value_normalizer" + "_" + str(current_timestep) + ".pt",
            )

    def restore(self):
        """Restore model parameters."""
        if self.share_param:
            for agent_id in range(self.num_agents):
                policy_actor_state_dict = torch.load(
                    str(self.algo_args["train"]["model_dir"])
                    + "/actor_agent"
                    + '0'
                    + ".pt",
                weights_only = True
                )
                self.actor[agent_id].actor.load_state_dict(policy_actor_state_dict)
        else:
            for agent_id in range(self.num_agents):
                policy_actor_state_dict = torch.load(
                    str(self.algo_args["train"]["model_dir"])
                    + "/actor_agent"
                    + str(agent_id)
                    + ".pt"
                )
                self.actor[agent_id].actor.load_state_dict(policy_actor_state_dict)
        if not self.algo_args["render"]["use_render"]:
            policy_critic_state_dict = torch.load(
                str(self.algo_args["train"]["model_dir"]) + "/critic_agent" + ".pt"
            )
            self.critic.critic.load_state_dict(policy_critic_state_dict)
            if self.value_normalizer is not None:
                value_normalizer_state_dict = torch.load(
                    str(self.algo_args["train"]["model_dir"])
                    + "/value_normalizer"
                    + ".pt"
                )
                self.value_normalizer.load_state_dict(value_normalizer_state_dict)

    def close(self):
        """Close environment, writter, and logger."""
        if self.algo_args["render"]["use_render"]:
            self.envs.close()
        else:
            self.envs.close()
            if self.algo_args["eval"]["use_eval"] and self.eval_envs is not self.envs:
                self.eval_envs.close()
            self.writter.export_scalars_to_json(str(self.log_dir + "/summary.json"))
            self.writter.close()
            self.logger.close()

    def monitor_gradients_and_parameters(self):
        """ä¿ç•™ä½†ç®€åŒ–"""
        if not hasattr(self, 'gradient_monitor'):
            return

        try:
            episode = getattr(self, 'episode', 0)
            current_step = episode * self.algo_args["train"]["episode_length"] * \
                          self.algo_args["train"]["n_rollout_threads"]

            # ç›‘æŽ§Actorç½‘ç»œ
            if self.share_param:
                self.gradient_monitor.monitor_gradients(self.actor[0].actor, "shared_actor", current_step)
                self.gradient_monitor.monitor_parameters(self.actor[0].actor, "shared_actor", current_step)
                self.gradient_monitor.monitor_learning_rate(self.actor[0].actor_optimizer, "shared_actor", current_step)
            else:
                for agent_id in range(self.num_agents):
                    model_name = f"actor_agent_{agent_id}"
                    self.gradient_monitor.monitor_gradients(self.actor[agent_id].actor, model_name, current_step)
                    self.gradient_monitor.monitor_parameters(self.actor[agent_id].actor, model_name, current_step)
                    self.gradient_monitor.monitor_learning_rate(self.actor[agent_id].actor_optimizer, model_name, current_step)

            # ç›‘æŽ§Criticç½‘ç»œ
            self.gradient_monitor.monitor_gradients(self.critic.critic, "critic", current_step)
            self.gradient_monitor.monitor_parameters(self.critic.critic, "critic", current_step)
            self.gradient_monitor.monitor_learning_rate(self.critic.critic_optimizer, "critic", current_step)

            self.gradient_monitor.increment_update_count()

        except Exception as e:
            print(f"âš ï¸ Monitoring failed for episode {episode}: {e}")